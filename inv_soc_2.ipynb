{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEmd4ksvBgna"
      },
      "source": [
        "___\n",
        "\n",
        "# **Analisis e Interpretacion**\n",
        "\n",
        "En esta parte del esquema programatico vamos a definir los pasos necesarios para profundizar el analisis hecho, de forma exhaustiva, y, al mismo tiempo, lograr una interpretacion adecuada de los datos. Como mencionamos en la metodologia del presente proyecto [aplicaciones sociologicas con inteligencia artifical](inv_soc_1.ipynb), es necesario proceder a realizar un analisis de los temas extraidos asociados con el numero de likes y shares. Asimismo, incluir una interpretacion, luego de revisado los articulos pertinentes que nos ofreceran las perspectivas de nuestra interpretacion.\n",
        "\n",
        "1. **Sexta Tarea: Analisis de influencia de los temas**\n",
        "\n",
        "Lo primero sera la realizacion de un analisis del peso de los temas de acuerdo a los likes y shares asociados. Con esto, podremos conocer la influencia de determinados temas en la opinion general de la comunidad de Twitter.\n",
        "\n",
        "Para este analisis, como se menciono en la metodo [metodologia del proyecto](inv_soc_1.ipynb), se usara el modelo **`\"llama3-8b-8192\"`** mediante la instanciacion con [Groq](https://console.groq.com/home). Transformamos datos crudos de engagement en una tabla resumida, se la presenta a un LLM con instrucciones claras y específicas, y luego captura las interpretaciones y respuestas del LLM para documentarlas.\n",
        "\n",
        "Como detallaremos en la seccion correspondiente, en este analisis preparamos los datos de Engagement en forma tabular para que pudieran ser procesados por LLM.\n",
        "\n",
        "- **Preparación de los Datos (El \"Conteo\" con Pandas)**: Primero, tus datos brutos de engagement se agregan y resumen en una tabla. Esto implica contar las apariciones de cada tema y sumar los likes y shares que recibieron. Pandas hace el trabajo pesado aquí, organizando toda esta información cuantitativa de manera eficiente. El resultado es una tabla de fácil lectura, lista para el siguiente paso.\n",
        "\n",
        "- **El Modelo de Lenguaje Entra en Escena (Groq/LLaMA 3)**: Una vez que la tabla con los datos resumidos está lista, es el turno del Modelo de Lenguaje Grande (LLM), en este caso, LLaMA 3 de Groq. Su función no es realizar los cálculos que ya hizo Pandas, sino entender y razonar sobre la información que le presentas. El LLM \"lee\" la tabla y es capaz de interpretar sus patrones y responder a tus preguntas.\n",
        "\n",
        "- **La Orquestación de LangChain**: Para que el LLM pueda entender tus inquietudes y usar la tabla de datos, necesitamos una plantilla de instrucciones (un prompt) que le diga cómo actuar (por ejemplo, como un experto en análisis de redes sociales). LangChain es el que orquesta todo:\n",
        "\n",
        "  - Toma tu tabla de datos preparada y tu pregunta específica.\n",
        "  - Combina esto con la plantilla de instrucciones.\n",
        "  - Envía todo al LLM de Groq.\n",
        "   Recibe la respuesta generada por el LLM.\n",
        "\n",
        "De esta manera hicimos un conteo de los temas sumando los resultados de sus apariciones, conjuntamente con los likes y shares. Esto genera una agregacion de esos datos para que pueda ser procesado por nuesto modelo de lenguaje.\n",
        "\n",
        "En resumen, Pandas se encarga del trabajo pesado y eficiente de la agregación cuantitativa (el \"conteo\"), y el LLM se encarga de la interpretación cualitativa y la extracción de insights en lenguaje natural a partir de esos resultados ya procesados.\n",
        "\n",
        "2. **Septima Tarea: Interpretacion de los resultados**\n",
        "\n",
        "Despues, empieza la fase final del proyecto. La fase final del proyecto se enfoca en derivar insights significativos del conjunto de datos procesado, ligando los hallazgos empíricos con un marco conceptual sólido. Para ello, se emplearon modelos avanzados de lenguaje de Mistral y Google Gemini, orquestados por LlamaIndex y LangChain, en un proceso de dos etapas: la caracterización conceptual de documentos y la interpretación contextualizada de la influencia de los temas.\n",
        "\n",
        "La busqueda de los articulos cientificos es un paso necesario en la interpretacion de los reultados. Por dos razones, segun el articulo buscado crearemos un banco conceptual, con el cual, mediante el uso de documentos externos, el modelo de inteligencia artificial podra obtener los conocimientos especificos para nuestras interpretaciones.\n",
        "\n",
        "Para ello vamos aplicar un modelo RAG (Retrieval-Augmented Generation). El modeo RAG permite que un LLM acceda a una base de conocimiento externa (los documentos seleccionados) y se use esa información para generar respuestas. El LLM no \"aprende\" los documentos en sus pesos, sino que los \"consulta\" en tiempo real cuando necesita responder a una pregunta.\n",
        "\n",
        "Para este procesamiento utilizamos los modelos de [Mistral](https://docs.mistral.ai/api/.). Esta tarea se ejecutó de la siguiente forma:\n",
        "\n",
        "- **Vectorización de Embeddings para Documentos**: la vectorizacion de las palabras y cadenas de texto para que sea crucial la interpretacion semantica de las palabras y las frases segun el propio contexto de los documentos. La vectorizacion es clave para la extraccion de los conceptos clave solicitados en nuestros prompts.\n",
        "\n",
        "- **Caracterización Conceptual con LlamaIndex**: con la indexacion de los documentos externos mediante [LlamaIndex](https://www.llamaindex.ai/) pudimos realziar interacciones con el modelo **`\"mistral-large-latest\"`**, para conseguir la respuestas a nuestras consultas orientadas a la conceptualizacion clave de nuestras fuentes de informacion externa.\n",
        "\n",
        "- **Extracción y Registro de Definiciones**: Finalmente, extrajimos las definiciones clave esenciales para las perspectivas sociológica y de marketing digital, registrándolas en un documento de texto (**`\"informacion_extraida_de_pdfs.txt\"`**).\n",
        "\n",
        "Con este **banco conceptual** se pudo realizar las interpretaciones de la influencia de los temas. Es decir, interpretar los resultados de los analisis, unicamente con los conceptos claves esxtraidos de nuestros articulos cientificos.\n",
        "\n",
        "Este procedimeinto nos permitio afianzar los avances de la inteligencia artificial complementando distintos modelos en un esquema programatico de investigacion e interpretacion.\n",
        "\n",
        "- **Cadena RAG Orientada por Conceptos con LangChain**: En base a los conceptos y resúmenes clave obtenidos en la fase anterior, configuramos una cadena RAG (Generación Aumentada por Recuperación) utilizando [LangChain](https://www.langchain.com/). A esta cadena RAG se le suministraron las orientaciones interpretativas necesarias, basadas en nuestro marco conceptual, para analizar y contextualizar los resultados de nuestro estudio.\n",
        "\n",
        "- **Embeddings para Consultas y Contexto**: Para la operación efectiva de la cadena RAG, fue indispensable generar los embeddings de nuestras consultas (las preguntas o solicitudes de interpretación que hacíamos al modelo) y del contexto relevante (fragmentos de nuestros resultados de análisis y los conceptos del marco conceptual). Para esta tarea, empleamos el modelo de embedding **`\"models/embedding-001\"`**. Estos embeddings permitieron a la cadena RAG identificar y recuperar la información más pertinente para cada solicitud.\n",
        "\n",
        "- **Modelo de Interpretación**: La fase de interpretación se llevó a cabo con el modelo **`\"gemini-1.5-flash-latest\"`**. Dentro de la cadena RAG orquestada por [LangChain](https://www.langchain.com/), este modelo recibía los conceptos clave previamente definidos (en el marco_conceptual, **`\"informacion_extraida_de_pdfs.txt\"`**). De esta manera, el modelo interpretó los resultados registrados en nuestro documento de análisis (e.g.,**`\"llm_responses_engagement_analysis.txt\"`**), a la luz de los conceptos definidos en el paso previo (**`\"informacion_extraida_de_pdfs.txt\"`**). Este proceso garantiza una interpretación guiada y basada en el conocimiento específico del dominio.\n",
        "\n",
        "El esquema programatico nos permitira hallar conclusiones sociologicas y de marketing digital en medio de la interaccion entre la inteligencia artifical y la experticie de las ciencias sociales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He5KDg5j1jTt"
      },
      "source": [
        "## **Sexta Tarea: Analisis de influencia de los temas**\n",
        "\n",
        "En esta parte detallaremos nuestras configuraciones del modelo de analisis y agregacion de datos para una interpretacion de Inteligencia Artificial sobre el peso de los temas de nuestro conjunto de datos.\n",
        "\n",
        "\n",
        "La antesala de nuestra conversacion con el modelo de inteligencia artificial fue la agregacion de datos en un formato accesible para nuestro modelo **`\"llama3-8b-8192\"`**, y que esta agregacion de dato incluyera tanto el conteo de los temas como los likes o shares asociados.\n",
        "\n",
        "La estructura de nuestra funcion se enfoco en operacionalizar las labores de conteo y agregacion de datos. Mediante funciones basicas de PANDAS.\n",
        "\n",
        "- Se crea una columna para la agregacion del conteo por tema\n",
        "- Se realizan el conteo de los temas que gracias a la operación explode(), cada fila de este DataFrame ya representa un único tema.\n",
        "- Agregación de Métricas de Engagement para cada tema único identificado.\n",
        "- El resultado final, engagement_by_topic, es una tabla limpia y resumida. Cada fila de esta tabla representa un tema único, y las columnas muestran el total de likes, el total de shares y el número total de posts asociados a ese tema, ordenados por popularidad. Esta es la tabla que, en un paso posterior, alimentarás a tu LLM para que la interprete.\n",
        "\n",
        "Por este motivo, posteriormente definimos nuestro modelo de analista de datos de las redes sociales para que pudiera analizar la informacion segun las caracteristicas de la tabla de engagement previa, con la siguiente configuracion de la plantilla de instrucciones:\n",
        "\n",
        "  >\"Eres un analista de datos experto especializado en redes sociales.\n",
        "   Te proporcionaré una tabla de engagement por tema,\n",
        "   que incluye el total de likes,\n",
        "   total de shares y el número de publicaciones para cada tema.\n",
        "   Tu tarea es analizar esta tabla y responder preguntas específicas sobre los temas principales\".\n",
        "\n",
        "Con ese modelo buscabamos obtener un analisis de la influencia de los temas mediante la consulta a la tabla de Engagement por Tema segun el total de likes, total de shares, y el número de posts. Por esa razon, para obtener ua respuesta satisfactoria de nuestro  analisis de redes sociales especificamos las siguientes preguntas:\n",
        "  \n",
        "    ¿Cuáles son los 200 temas principales por número de likes? Para cada uno, indica su nombre,\n",
        "    el total de likes y el total de shares,\n",
        "    ¿Cuántos posts se asocian a los 200 temas más populares en términos de likes?,\n",
        "    ¿Cuántos posts se asocian a los 200 temas más populares en términos de shares,\n",
        "    ¿Cuál es el tema con menos shares entre los 200 temas principales por likes?\n",
        "    Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
        "\n",
        "Las respuestas a estas solicitudes fueron registradas en un documento para que pudiera ser interpretada por nuestro agente final. Pero, claro esta, previamente, se requeriria de la construccion de nuestro **banco conceptual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJMeURoWPUfj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/antropic2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrMB3usF13vs",
        "outputId": "8327153b-c7b9-4ef1-c849-d6ec2ff50b95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['author', 'content', 'country', 'date_time', 'id', 'language',\n",
              "       'latitude', 'longitude', 'number_of_likes', 'number_of_shares',\n",
              "       'hashtags', 'urls', 'temas'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msntuGjd3VGo",
        "outputId": "0ae5b044-d902-4b62-9a1a-c3c6437d8ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.63)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.26.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.14.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.4.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading groq-0.26.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.26.0 langchain-groq-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSVlEv2tWNza"
      },
      "source": [
        "###  Creacion de la funcion de agregacion y analisis de temas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px9h9GR_45P8",
        "outputId": "94b3599a-44f5-4547-f96a-910d0af67688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asegurando que la columna 'temas' es una lista real...\n",
            "\n",
            "Preparando datos de engagement por tema con Pandas...\n",
            "Contando palabras en cada tema individual de la columna 'temas'...\n",
            "Primeras filas del DataFrame explotado con el conteo de palabras por tema:\n",
            "| temas        |   word_count_per_topic |\n",
            "|:-------------|-----------------------:|\n",
            "| friendship   |                      1 |\n",
            "| social media |                      2 |\n",
            "| fashion      |                      1 |\n",
            "| television   |                      1 |\n",
            "| social media |                      2 |\n",
            "Datos de los temas principales (primeras 5 filas):\n",
            "| temas         |   total_likes |   total_shares |   number_of_posts |\n",
            "|:--------------|--------------:|---------------:|------------------:|\n",
            "| social media  |   1.60262e+07 |    9.4255e+06  |              1350 |\n",
            "| music         |   6.0706e+06  |    3.84532e+06 |               470 |\n",
            "| love          |   4.8109e+06  |    2.3783e+06  |               230 |\n",
            "| communication |   2.26858e+06 |    1.18134e+06 |               172 |\n",
            "| gratitude     |   1.79676e+06 |    1.08938e+06 |               128 |\n",
            "\n",
            "--- Haciendo preguntas a Groq sobre el engagement por tema ---\n",
            "\n",
            "Pregunta 1: ¿Cuáles son los 200 temas principales por número de likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Después de analizar la tabla, he identificado los 200 temas principales por número de likes. A continuación, te proporciono la lista con el nombre del tema, el total de likes y el total de shares:\n",
            "\n",
            "**Nota:** La tabla se ordena por el número de likes en orden descendente.\n",
            "\n",
            "1. Social media - 16,026,200 likes - 9,425,000 shares\n",
            "2. Music - 6,070,600 likes - 3,845,200 shares\n",
            "3. Love - 4,810,900 likes - 2,378,300 shares\n",
            "4. Communication - 2,268,580 likes - 1,181,340 shares\n",
            "5. Gratitude - 1,796,760 likes - 1,089,380 shares\n",
            "6. Entertainment - 1,723,750 likes - 1,001,850 shares\n",
            "7. Fashion - 1,321,970 likes - 623,423 shares\n",
            "8. Celebrity - 1,213,020 likes - 692,063 shares\n",
            "9. Relationships - 1,132,930 likes - 704,245 shares\n",
            "10. Relationship - 1,121,030 likes - 610,932 shares\n",
            "... (continúa)\n",
            "\n",
            "**200.** Patriotism - 121,795 likes - 66,615 shares\n",
            "\n",
            "**Observación:** Los temas con un número de likes muy alto son \"Social media\", \"Music\", \"Love\", \"Communication\", \"Gratitude\", \"Entertainment\", \"Fashion\", \"Celebrity\", \"Relationships\" y \"Relationship\".\n",
            "\n",
            "Pregunta 2: ¿Cuántos posts se asocian a los 200 temas más populares en términos de likes?\n",
            "Para responder a esta pregunta, puedo ordenar la tabla por el número de likes en orden descendente y luego sumar los posts de los 200 temas más populares.\n",
            "\n",
            "Después de ordenar la tabla, puedo ver que los 200 temas más populares en términos de likes son:\n",
            "\n",
            "1. social media - 1,602,620,000 likes\n",
            "2. music - 6,070,600 likes\n",
            "3. love - 4,810,900 likes\n",
            "...\n",
            "200. patriotism - 121,795 likes\n",
            "\n",
            "Luego, puedo sumar los posts de estos 200 temas:\n",
            "\n",
            "1350 + 470 + 230 + ... + 2 = 43,311\n",
            "\n",
            "Por lo tanto, los 200 temas más populares en términos de likes se asocian con un total de 43,311 posts.\n",
            "\n",
            "Pregunta 3: ¿Cuántos posts se asocian a los 200 temas más populares en términos de shares?\n",
            "Para responder a esta pregunta, puedo ordenar la tabla por el número de shares en orden descendente y luego sumar los posts de los 200 temas más populares.\n",
            "\n",
            "Después de ordenar la tabla, puedo ver que los 200 temas más populares en términos de shares son:\n",
            "\n",
            "1. social media - 9.4255e+06 shares\n",
            "2. music - 3.84532e+06 shares\n",
            "3. love - 2.3783e+06 shares\n",
            "...\n",
            "200. patriotism - 66615 shares\n",
            "\n",
            "Luego, puedo sumar los posts de estos 200 temas:\n",
            "\n",
            "1350 + 470 + 230 + ... + 2 = 14321\n",
            "\n",
            "Por lo tanto, los 200 temas más populares en términos de shares se asocian con un total de 14321 posts.\n",
            "\n",
            "Pregunta 4: ¿Cuál es el tema con menos shares entre los 200 temas principales por likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Después de analizar la tabla, encontré que el tema con menos shares entre los 200 temas principales por likes es \"anxiety\" con 86,453 shares.\n",
            "\n",
            "Aquí está la información para cada uno de los temas:\n",
            "\n",
            "* Tema: anxiety\n",
            "* Total de likes: 215,548\n",
            "* Total de shares: 86,453\n",
            "\n",
            "Es importante destacar que esta información se basa en la tabla proporcionada y puede no reflejar la realidad total de los temas y sus respectivos engagement.\n",
            "\n",
            "--- Todas las preguntas de engagement por tema respondidas por Groq. ---\n",
            "\n",
            "Las respuestas del LLM se han guardado exitosamente en 'llm_responses_engagement_analysis.txt' en formato TXT.\n"
          ]
        }
      ],
      "source": [
        "df_completo = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# --- Asegurar que la columna 'temas' contiene listas reales ---\n",
        "# Es crucial que esta columna sea una lista de Python para el explode()\n",
        "def safe_literal_eval(val):\n",
        "    try:\n",
        "        # Solo intentar evaluar si es una cadena y parece una lista\n",
        "        if isinstance(val, str) and val.strip().startswith('[') and val.strip().endswith(']'):\n",
        "            return ast.literal_eval(val)\n",
        "        # Si ya es una lista, o no es una cadena que parezca lista, devolverla tal cual\n",
        "        return val\n",
        "    except (ValueError, SyntaxError):\n",
        "        return [] # Retorna lista vacía para errores o si no es una lista válida\n",
        "\n",
        "print(\"Asegurando que la columna 'temas' es una lista real...\")\n",
        "if 'temas' in df_completo.columns:\n",
        "    df_completo['temas'] = df_completo['temas'].apply(safe_literal_eval)\n",
        "else:\n",
        "    print(\"Advertencia: La columna 'temas' no se encontró en el DataFrame.\")\n",
        "    # Si no existe, crea una columna vacía para evitar errores posteriores si el código lo espera\n",
        "    df_completo['temas'] = [[] for _ in range(len(df_completo))]\n",
        "\n",
        "\n",
        "# --- 1. Configurar la clave API de Groq ---\n",
        "if 'GROQ_API_KEY' not in os.environ:\n",
        "    try:\n",
        "        groq_api_key = getpass.getpass('Introduce tu clave de la API de Groq: ')\n",
        "        os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo obtener la clave API de Groq: {e}\")\n",
        "        print(\"Por favor, configura la variable de entorno 'GROQ_API_KEY' o ingresa la clave manualmente.\")\n",
        "        exit()\n",
        "\n",
        "# --- 2. Inicializar el modelo de Groq ---\n",
        "llm_groq = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.1)\n",
        "\n",
        "# --- 3. Preparar los datos de engagement por tema usando Pandas (¡Crucial!) ---\n",
        "# Este paso agrega los datos para que el LLM los pueda interpretar fácilmente.\n",
        "print(\"\\nPreparando datos de engagement por tema con Pandas...\")\n",
        "\n",
        "# Explota el DataFrame por la columna 'temas'\n",
        "df_exploded_temas_engagement = df_completo.explode('temas')\n",
        "\n",
        "# Limpia las filas donde 'temas' es nulo o una cadena vacía\n",
        "df_exploded_temas_engagement = df_exploded_temas_engagement[\n",
        "    df_exploded_temas_engagement['temas'].notna() &\n",
        "    (df_exploded_temas_engagement['temas'] != '')\n",
        "]\n",
        "\n",
        "# AÑADE AQUÍ EL CONTEO DE PALABRAS PARA CADA TEMA INDIVIDUAL\n",
        "print(\"Contando palabras en cada tema individual de la columna 'temas'...\")\n",
        "df_exploded_temas_engagement['word_count_per_topic'] = df_exploded_temas_engagement['temas'].apply(lambda x: len(str(x).split()))\n",
        "print(\"Primeras filas del DataFrame explotado con el conteo de palabras por tema:\")\n",
        "print(df_exploded_temas_engagement[['temas', 'word_count_per_topic']].head().to_markdown(index=False))\n",
        "\n",
        "\n",
        "# Agrupa por 'temas' y calcula las métricas de engagement\n",
        "engagement_by_topic = df_exploded_temas_engagement.groupby('temas').agg(\n",
        "    total_likes=('number_of_likes', 'sum'),\n",
        "    total_shares=('number_of_shares', 'sum'),\n",
        "    number_of_posts=('id', 'count')\n",
        "    # Si quisieras la media de palabras por tema (aunque no tiene sentido aquí), la añadirías:\n",
        "    # average_words_in_topic=('word_count_per_topic', 'mean')\n",
        ").sort_values(by='total_likes', ascending=False)\n",
        "\n",
        "# Tomamos los 20 temas principales para dar suficiente contexto al LLM,\n",
        "# aunque la pregunta es por los 10 primeros. El LLM puede filtrar.\n",
        "top_topics_data = engagement_by_topic.head(200) # Obtener más de 10 para dar margen al LLM\n",
        "print(\"Datos de los temas principales (primeras 5 filas):\")\n",
        "print(top_topics_data.head().to_markdown(index=True)) # Imprime en formato Markdown para visualizar\n",
        "\n",
        "# --- 4. Definir el prompt para el LLM ---\n",
        "# Pasamos la tabla agregada al LLM en formato Markdown.\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"\"\"Eres un analista de datos experto especializado en redes sociales. Te proporcionaré una tabla de engagement por tema, que incluye el total de likes, total de shares y el número de publicaciones para cada tema. Tu tarea es analizar esta tabla y responder preguntas específicas sobre los temas principales.\n",
        "\n",
        "          **Tabla de Engagement por Tema (total de likes, total de shares, número de posts):**\n",
        "          {engagement_table}\n",
        "\n",
        "          Responde a las siguientes preguntas basándote únicamente en la tabla proporcionada. Sé conciso y directo.\"\"\"),\n",
        "        (\"human\", \"Pregunta: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- 5. Crear la cadena LLM ---\n",
        "llm_chain = prompt_template | llm_groq\n",
        "\n",
        "# --- 6. Definir las preguntas específicas sobre el engagement por tema ---\n",
        "questions = [\n",
        "    \"¿Cuáles son los 200 temas principales por número de likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\",\n",
        "    \"¿Cuántos posts se asocian a los 200 temas más populares en términos de likes?\",\n",
        "    \"¿Cuántos posts se asocian a los 200 temas más populares en términos de shares?\",\n",
        "    \"¿Cuál es el tema con menos shares entre los 200 temas principales por likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\",\n",
        "    # Puedes añadir más preguntas que quieras hacer al LLM sobre esta tabla\n",
        "]\n",
        "\n",
        "# --- 7. Iterar a través de las preguntas y obtener las respuestas del LLM ---\n",
        "llm_responses = {}\n",
        "\n",
        "print(\"\\n--- Haciendo preguntas a Groq sobre el engagement por tema ---\")\n",
        "for i, q in enumerate(questions):\n",
        "    print(f\"\\nPregunta {i+1}: {q}\")\n",
        "    try:\n",
        "        # Pasamos la tabla de engagement_by_topic convertida a Markdown como contexto.\n",
        "        response = llm_chain.invoke({\n",
        "            \"engagement_table\": top_topics_data.to_markdown(index=True), # 'index=True' para incluir el nombre del tema\n",
        "            \"question\": q\n",
        "        })\n",
        "        llm_response_content = response.content\n",
        "        llm_responses[q] = llm_response_content\n",
        "        print(llm_response_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando la pregunta '{q}': {e}\")\n",
        "        llm_responses[q] = f\"Error: {e}\"\n",
        "\n",
        "print(\"\\n--- Todas las preguntas de engagement por tema respondidas por Groq. ---\")\n",
        "\n",
        "# --- 8. Guardar los resultados de las respuestas del LLM en un archivo TXT ---\n",
        "output_filename = \"llm_responses_engagement_analysis.txt\"\n",
        "try:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        for q, a in llm_responses.items():\n",
        "            f.write(f\"Pregunta: {q}\\n\")\n",
        "            f.write(f\"Respuesta: {a}\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\\n\") # Separador para mejor lectura\n",
        "    print(f\"\\nLas respuestas del LLM se han guardado exitosamente en '{output_filename}' en formato TXT.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError al guardar las respuestas del LLM en '{output_filename}': {e}\")\n",
        "\n",
        "# Puedes revisar el diccionario de respuestas:\n",
        "# print(\"\\nResumen de todas las respuestas del LLM:\")\n",
        "# for q, a in llm_responses.items():\n",
        "#     print(f\"P: {q}\\nR: {a}\\n---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtu8E3Kmfwzc"
      },
      "source": [
        "---\n",
        "Resumen\n",
        "\n",
        "En el anterior codigo se obtiene el analisis de texto mediante un registro en un documento txt. Con est documento vemos el tema anxiety con mayor peso, asi como la tabla de engagemente para los 200 temas principales de los cuales obtenemos las siguientes impresiones:\n",
        "\n",
        "1. Social media - 16,026,200 likes - 9,425,000 shares\n",
        "2. Music - 6,070,600 likes - 3,845,200 shares\n",
        "3. Love - 4,810,900 likes - 2,378,300 shares\n",
        "4. Communication - 2,268,580 likes - 1,181,340 shares\n",
        "5. Gratitude - 1,796,760 likes - 1,089,380 shares\n",
        "6. Entertainment - 1,723,750 likes - 1,001,850 shares\n",
        "7. Fashion - 1,321,970 likes - 623,423 shares\n",
        "8. Celebrity - 1,213,020 likes - 692,063 shares\n",
        "9. Relationships - 1,132,930 likes - 704,245 shares\n",
        "10. Relationship - 1,121,030 likes - 610,932 shares\n",
        "\n",
        "De modo que, estos representan los temas con mayor peso en la opinion publica de las publicaciones observadas. Nuestro proximo paso sera realizar la interpretacion del analisis de influencia de temas segun las dos perspectivas de nuestro esquemas conceptuales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l8vx79qNXy2"
      },
      "source": [
        "#### Importacion de las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2qsgA_O7WF6u",
        "outputId": "8e196cfe-9c94-4429-d393-a6b101aa736a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.63)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.5)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (1.0.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.6.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.72.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.24.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb>=1.0.9->langchain-chroma) (0.45.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.32.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core langchain-community langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_ZGbKC_WWSe_",
        "outputId": "74e0dfbe-c6e5-4ff7-ee48-aadbc384703e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.63)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ubcTtB6HWO-0",
        "outputId": "23d53982-5276-4a7b-f56a-aed84b16699c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (4.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jcQ-FPeOcy-j",
        "outputId": "454c4e4b-6d89-443a-acca-709fe9f9296a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (4.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDW8SZ31hale"
      },
      "source": [
        "#### Visualizacion de los modelos gemini disponibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "WulovgNqX7Tn",
        "outputId": "0ea12626-ec0b-49ef-f7c8-0a426edf1bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelos disponibles:\n",
            "  Nombre: models/gemini-1.0-pro-vision-latest\n",
            "  Nombre a mostrar: Gemini 1.0 Pro Vision\n",
            "  Descripción: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 12288\n",
            "  Límite de tokens de salida: 4096\n",
            "------------------------------\n",
            "  Nombre: models/gemini-pro-vision\n",
            "  Nombre a mostrar: Gemini 1.0 Pro Vision\n",
            "  Descripción: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 12288\n",
            "  Límite de tokens de salida: 4096\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-pro-latest\n",
            "  Nombre a mostrar: Gemini 1.5 Pro Latest\n",
            "  Descripción: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 2000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-pro-001\n",
            "  Nombre a mostrar: Gemini 1.5 Pro 001\n",
            "  Descripción: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "  Límite de tokens de entrada: 2000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-pro-002\n",
            "  Nombre a mostrar: Gemini 1.5 Pro 002\n",
            "  Descripción: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "  Límite de tokens de entrada: 2000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-pro\n",
            "  Nombre a mostrar: Gemini 1.5 Pro\n",
            "  Descripción: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 2000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-latest\n",
            "  Nombre a mostrar: Gemini 1.5 Flash Latest\n",
            "  Descripción: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-001\n",
            "  Nombre a mostrar: Gemini 1.5 Flash 001\n",
            "  Descripción: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-001-tuning\n",
            "  Nombre a mostrar: Gemini 1.5 Flash 001 Tuning\n",
            "  Descripción: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createTunedModel']\n",
            "  Límite de tokens de entrada: 16384\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash\n",
            "  Nombre a mostrar: Gemini 1.5 Flash\n",
            "  Descripción: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-002\n",
            "  Nombre a mostrar: Gemini 1.5 Flash 002\n",
            "  Descripción: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-8b\n",
            "  Nombre a mostrar: Gemini 1.5 Flash-8B\n",
            "  Descripción: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
            "  Métodos de generación soportados: ['createCachedContent', 'generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-8b-001\n",
            "  Nombre a mostrar: Gemini 1.5 Flash-8B 001\n",
            "  Descripción: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
            "  Métodos de generación soportados: ['createCachedContent', 'generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-8b-latest\n",
            "  Nombre a mostrar: Gemini 1.5 Flash-8B Latest\n",
            "  Descripción: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
            "  Métodos de generación soportados: ['createCachedContent', 'generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-8b-exp-0827\n",
            "  Nombre a mostrar: Gemini 1.5 Flash 8B Experimental 0827\n",
            "  Descripción: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-1.5-flash-8b-exp-0924\n",
            "  Nombre a mostrar: Gemini 1.5 Flash 8B Experimental 0924\n",
            "  Descripción: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1000000\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-pro-exp-03-25\n",
            "  Nombre a mostrar: Gemini 2.5 Pro Experimental 03-25\n",
            "  Descripción: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-pro-preview-03-25\n",
            "  Nombre a mostrar: Gemini 2.5 Pro Preview 03-25\n",
            "  Descripción: Gemini 2.5 Pro Preview 03-25\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-flash-preview-04-17\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview 04-17\n",
            "  Descripción: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-flash-preview-05-20\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview 05-20\n",
            "  Descripción: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-flash-preview-04-17-thinking\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview 04-17 for cursor testing\n",
            "  Descripción: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-pro-preview-05-06\n",
            "  Nombre a mostrar: Gemini 2.5 Pro Preview 05-06\n",
            "  Descripción: Preview release (May 6th, 2025) of Gemini 2.5 Pro\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-pro-preview-06-05\n",
            "  Nombre a mostrar: Gemini 2.5 Pro Preview\n",
            "  Descripción: Preview release (June 5th, 2025) of Gemini 2.5 Pro\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-exp\n",
            "  Nombre a mostrar: Gemini 2.0 Flash Experimental\n",
            "  Descripción: Gemini 2.0 Flash Experimental\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash\n",
            "  Nombre a mostrar: Gemini 2.0 Flash\n",
            "  Descripción: Gemini 2.0 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-001\n",
            "  Nombre a mostrar: Gemini 2.0 Flash 001\n",
            "  Descripción: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-lite-001\n",
            "  Nombre a mostrar: Gemini 2.0 Flash-Lite 001\n",
            "  Descripción: Stable version of Gemini 2.0 Flash Lite\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-lite\n",
            "  Nombre a mostrar: Gemini 2.0 Flash-Lite\n",
            "  Descripción: Gemini 2.0 Flash-Lite\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-lite-preview-02-05\n",
            "  Nombre a mostrar: Gemini 2.0 Flash-Lite Preview 02-05\n",
            "  Descripción: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-lite-preview\n",
            "  Nombre a mostrar: Gemini 2.0 Flash-Lite Preview\n",
            "  Descripción: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-pro-exp\n",
            "  Nombre a mostrar: Gemini 2.0 Pro Experimental\n",
            "  Descripción: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-pro-exp-02-05\n",
            "  Nombre a mostrar: Gemini 2.0 Pro Experimental 02-05\n",
            "  Descripción: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-exp-1206\n",
            "  Nombre a mostrar: Gemini Experimental 1206\n",
            "  Descripción: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview 04-17\n",
            "  Descripción: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-thinking-exp\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview 04-17\n",
            "  Descripción: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.0-flash-thinking-exp-1219\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview 04-17\n",
            "  Descripción: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-flash-preview-tts\n",
            "  Nombre a mostrar: Gemini 2.5 Flash Preview TTS\n",
            "  Descripción: Gemini 2.5 Flash Preview TTS\n",
            "  Métodos de generación soportados: ['countTokens', 'generateContent']\n",
            "  Límite de tokens de entrada: 32768\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemini-2.5-pro-preview-tts\n",
            "  Nombre a mostrar: Gemini 2.5 Pro Preview TTS\n",
            "  Descripción: Gemini 2.5 Pro Preview TTS\n",
            "  Métodos de generación soportados: ['countTokens', 'generateContent']\n",
            "  Límite de tokens de entrada: 65536\n",
            "  Límite de tokens de salida: 65536\n",
            "------------------------------\n",
            "  Nombre: models/learnlm-2.0-flash-experimental\n",
            "  Nombre a mostrar: LearnLM 2.0 Flash Experimental\n",
            "  Descripción: LearnLM 2.0 Flash Experimental\n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 1048576\n",
            "  Límite de tokens de salida: 32768\n",
            "------------------------------\n",
            "  Nombre: models/gemma-3-1b-it\n",
            "  Nombre a mostrar: Gemma 3 1B\n",
            "  Descripción: \n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 32768\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemma-3-4b-it\n",
            "  Nombre a mostrar: Gemma 3 4B\n",
            "  Descripción: \n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 32768\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemma-3-12b-it\n",
            "  Nombre a mostrar: Gemma 3 12B\n",
            "  Descripción: \n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 32768\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemma-3-27b-it\n",
            "  Nombre a mostrar: Gemma 3 27B\n",
            "  Descripción: \n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 131072\n",
            "  Límite de tokens de salida: 8192\n",
            "------------------------------\n",
            "  Nombre: models/gemma-3n-e4b-it\n",
            "  Nombre a mostrar: Gemma 3n E4B\n",
            "  Descripción: \n",
            "  Métodos de generación soportados: ['generateContent', 'countTokens']\n",
            "  Límite de tokens de entrada: 8192\n",
            "  Límite de tokens de salida: 2048\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Configura tu clave de API.\n",
        "# Es recomendable cargarla desde una variable de entorno para mayor seguridad.\n",
        "# Por ejemplo: os.environ.get(\"GOOGLE_API_KEY\")\n",
        "# Asegúrate de que tu clave de API esté configurada correctamente.\n",
        "# Puedes obtener una clave de API en: https://makersuite.google.com/keys\n",
        "API_KEY = \"GEMINI_API_KEY # ¡Reemplaza con tu clave de API real!\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Lista todos los modelos disponibles\n",
        "print(\"Modelos disponibles:\")\n",
        "for m in genai.list_models():\n",
        "    # Filtra para mostrar solo los modelos que soportan el método \"generateContent\"\n",
        "    # y que tienen un nombre que comienza con \"models/\" (modelos de uso general)\n",
        "    if \"generateContent\" in m.supported_generation_methods and m.name.startswith(\"models/\"):\n",
        "        print(f\"  Nombre: {m.name}\")\n",
        "        print(f\"  Nombre a mostrar: {m.display_name}\")\n",
        "        print(f\"  Descripción: {m.description}\")\n",
        "        print(f\"  Métodos de generación soportados: {m.supported_generation_methods}\")\n",
        "        print(f\"  Límite de tokens de entrada: {m.input_token_limit}\")\n",
        "        print(f\"  Límite de tokens de salida: {m.output_token_limit}\")\n",
        "        print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CDF8puy2Uj4"
      },
      "source": [
        "---\n",
        "## **Septima Tarea: Interpretacion de los resultados**\n",
        "\n",
        "\n",
        "Como mencionamos esta fase constituyo en varias tareas especificas:\n",
        "- La busqueda de los documentos externos\n",
        "- La creacion del banco conceptuall\n",
        "- La interpretacion de los resultados de los analisis del peso de los temas basados en nuestro marco referencial\n",
        "\n",
        "Cada una de estas tareas especificas las detallaremos en las subsecciones correspondientes.\n",
        "\n",
        "En lineas generales, podemos mencionar que tuvimos que aplicar RAG (Retrieval-Augmented Generation). Esta estrategia de Inteligencia Artificial,nos permite que un LLM acceda a una base de conocimiento externa (tus documentos) y use esa información para generar respuestas. El LLM no \"aprende\" los documentos en sus pesos, sino que los \"consulta\" en tiempo real cuando necesita responder a nuestras solicitudes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qTjJyrvVknnD",
        "outputId": "b4c0d1b3-59d0-4a47-a762-ab2b6885809b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.41-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-llms-mistralai\n",
            "  Downloading llama_index_llms_mistralai-0.5.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting llama-index-embeddings-mistralai\n",
            "  Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl.metadata (696 bytes)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.9-py3-none-any.whl.metadata (438 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.41 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.41-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.7.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.4.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: mistralai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-mistralai) (1.8.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.84.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (3.11.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.41->llama-index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.41->llama-index)\n",
            "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.6.7)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.41->llama-index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.41->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.11.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.41->llama-index) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.17.2)\n",
            "Collecting llama-cloud==0.1.23 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.23-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.23->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.30-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (1.20.0)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.41->llama-index) (0.16.0)\n",
            "Collecting llama-cloud-services>=0.6.30 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.30-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.0.0->llama-index-llms-mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.41->llama-index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.41->llama-index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.41->llama-index) (3.2.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.41->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.30->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index) (24.2)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.41-py3-none-any.whl (7.1 kB)\n",
            "Downloading llama_index_llms_mistralai-0.5.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl (2.6 kB)\n",
            "Downloading llama_index_agent_openai-0.4.9-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.41-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.7.4-py3-none-any.whl (15 kB)\n",
            "Downloading llama_cloud-0.1.23-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.3/267.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.4.4-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
            "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_parse-0.6.30-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading llama_cloud_services-0.6.30-py3-none-any.whl (38 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, deprecated, colorama, aiosqlite, griffe, llama-cloud, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed aiosqlite-0.21.0 banks-2.1.2 colorama-0.4.6 deprecated-1.2.18 dirtyjson-1.0.8 griffe-1.7.3 llama-cloud-0.1.23 llama-cloud-services-0.6.30 llama-index-0.12.41 llama-index-agent-openai-0.4.9 llama-index-cli-0.4.3 llama-index-core-0.12.41 llama-index-embeddings-mistralai-0.3.0 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.4 llama-index-llms-mistralai-0.5.0 llama-index-llms-openai-0.4.4 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.30 pypdf-5.6.0 striprtf-0.0.26\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-llms-mistralai llama-index-embeddings-mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwf6amHevApM"
      },
      "source": [
        "### **La creacion del banco conceptual**\n",
        "\n",
        "Luego de la busqueda de los documentos externos, la creacion del banco conceptual se llevo a cabo con el siguiente procedimiento:\n",
        "\n",
        "1.  **Extracción de Texto del PDF (`extraer_texto_pdf`):** Mantenemos tu función `fitz` para obtener el contenido de cada PDF como texto plano.\n",
        "2.  **Carga de Documentos en LlamaIndex:** Cada texto de PDF se convierte en un `Document` de LlamaIndex.\n",
        "3.  **`VectorStoreIndex` por PDF:** Creamos un `VectorStoreIndex` para *cada PDF*. Esto nos permite hacer preguntas directamente sobre el contenido de cada documento.\n",
        "4.  **Prompt de Extracción Dirigida (`extraction_prompt_tmpl`):** Esta es la parte crucial. Creamos un `PromptTemplate` que le indica al LLM que su tarea es **extraer** información, definiciones y conceptos clave relacionados con un tema específico (`{topic}`, que será \"marketing\" o \"sociología\") *basándose solo en el contexto del documento*.\n",
        "    * Le pedimos que sea explícito y que incluso indique si no encuentra información.\n",
        "5.  **Consulta Específica (`query_engine.query`):** Para cada PDF, hacemos una consulta como \"Extrae definiciones, conceptos clave e información fundamental sobre **marketing** en redes sociales\" (o \"sociología\").\n",
        "6.  **`partial_format` del Prompt:** Usamos `.partial_format(topic=doc_type.lower())` para insertar dinámicamente \"marketing\" o \"sociología\" en el prompt, personalizando la solicitud de extracción para cada documento.\n",
        "7.  **Salida:** La información extraída se imprime en la consola y se guarda en un archivo `informacion_extraida_de_pdfs.txt`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uf1nT-byvBc",
        "outputId": "660dd6bf-49ee-4061-f5ee-d316d855910f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VA1kgpAMj55g",
        "outputId": "c2606155-af27-4852-a3d0-c6bf4c41eed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.41)\n",
            "Requirement already satisfied: llama-index-llms-mistralai in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: llama-index-embeddings-mistralai in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.9)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.41 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.41)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.7.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.4)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.6,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.9)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: mistralai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-mistralai) (1.8.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.84.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.11.5)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.41->llama-index) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.41->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud==0.1.23 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.23)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.23->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.6.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.30)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.41->llama-index) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.41->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.41->llama-index) (0.16.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.30 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.30)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.41->llama-index) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.0.0->llama-index-llms-mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.41->llama-index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.41->llama-index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.41->llama-index) (3.2.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.41->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.30->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.41->llama-index) (24.2)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.41->llama-index) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-llms-mistralai llama-index-embeddings-mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWosGxhTVXy-"
      },
      "source": [
        "De manera que el siguiente programa se encarga de la caracterización Conceptual con LlamaIndex y Mistral. El programa con LlamaIndex y Mistral se encarga de extraer texto de documentos PDF y luego utilizar LlamaIndex con modelos de Mistral AI para la identificacion y extraccion de definiciones y conceptos }relevantes para el marketing y la sociología, guardando estos hallazgos en un archivo de texto.\n",
        "\n",
        "Para lograr este bano conceptos realizamos lo siguiente:\n",
        "\n",
        "- **La Configuración Inicial**:\n",
        "\n",
        "Aseguramos que la clave de API para Mistral AI esté configurada, lo cual es esencial para autenticarte y usar sus modelos. Luego hicimos la configuración de LlamaIndex para mistral-large-latest como el Modelo de Lenguaje Grande (LLM) principal para la comprel uso de la extensión y extracción de texto, configurado con una baja \"temperatura\" para respuestas más directas.\n",
        "\n",
        "Luego, fue necesario el uso de mistral-embed como el modelo de embedding para convertir el texto en representaciones numéricas que LlamaIndex puede indexar y buscar eficientemente.\n",
        "\n",
        "- **Extracción de Texto de PDFs**:\n",
        "\n",
        "Definimos una función extraer_texto_pdf que utiliza la librería PyMuPDF (fitz) para leer un archivo PDF y extraer todo su contenido textual, página por página.\n",
        "Procesamiento y Extracción por Documento: El script itera a través de las rutas de tus dos PDFs (uno de marketing y otro de sociología).\n",
        "\n",
        "Para cada PDF extrajimos todo el texto usando la función extraer_texto_pdf.\n",
        "Creamos un objeto Document de LlamaIndex a partir de este texto, añadiendo metadatos como la fuente.\n",
        "\n",
        "\n",
        "- **Creación del Prompt (Plantilla de Instrucciones)**\n",
        "\n",
        "Luego se construye un VectorStoreIndex de LlamaIndex con este documento. Este índice convierte el texto en embeddings (usando mistral-embed) y los almacena de forma que sean rápidamente recuperables.\n",
        "\n",
        "Para que esto fuera posible creamos una plantilla de instrucciones. Definimos un PromptTemplate específico (extraction_prompt_tmpl). Este prompt le indica al LLM (Mistral-Large) que su tarea es extraer definiciones y conceptos clave exclusivamente del contexto proporcionado de las redes sociales, relacionado con el tipo de documento (marketing o sociología).\n",
        "\n",
        "\n",
        ">\" Eres un asistente experto en análisis de texto.\n",
        "   Basándote EXCLUSIVAMENTE en el siguiente contexto extraído de un documento,\n",
        "   identifica y extrae las definiciones clave, conceptos importantes o información fundamental\n",
        "   relacionada con la \"{topic}\" de las redes sociales.\n",
        "   Si no se encuentra información explícita sobre ello, indícalo.\n",
        "   Contexto del documento:\n",
        "   {context_str} Información clave extraída (Definiciones, conceptos, ideas principales sobre {topic})\"\n",
        "\n",
        "- **Configuración de la Cadena LLM (Conexión y Ejecución)**:\n",
        "\n",
        "Luego de la creacion de la plantilla configuramos un query_engine (motor de consulta) que utiliza el VectorStoreIndex. Este motor es el que realizará la búsqueda y síntesis. Se le actualiza el prompt con la plantilla de extracción para guiar su respuesta.\n",
        "\n",
        "La query consulta se construyo de la siguiente manera:\n",
        "\n",
        "  >\"Extrae definiciones, conceptos clave e información fundamental sobre {doc_type.lower()} en redes sociales.\"\n",
        "\n",
        "La ejecucion de una query (consulta) contra el query_engine (ej. \"Extrae definiciones... sobre marketing en redes sociales\"), permitio que la cadena de LlamaIndex (índice -> recuperación -> LLM) buscara los fragmentos más relevantes del PDF (usando los embeddings), los pasa al mistral-large-latest junto con el prompt, y el LLM sintetiza la información clave. La información extraída se almacena en el diccionario extracted_info y, crucialmente, se guarda en el archivo informacion_extraida_de_pdfs.txt.\n",
        "\n",
        "En todo el proceso se incluye un robusto manejo de errores (try-except) para capturar y reportar cualquier problema durante el procesamiento de un PDF.\n",
        "En esencia, este código automatiza la lectura de tus artículos científicos en PDF, la creación de índices inteligentes de su contenido y la extracción dirigida de conceptos y definiciones relevantes para tu investigación sociológica y de marketing, creando así tu marco conceptual de referencia.\n",
        "\n",
        "\n",
        "         \n",
        "              -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aTnz4cdgvAxI",
        "outputId": "2ebed3c7-5507-4ad9-b6e7-c2468a3419f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Procesando PDF de Marketing ---\n",
            "Información extraída de Marketing:\n",
            "Basándome exclusivamente en el contexto proporcionado, he identificado y extraído las definiciones clave, conceptos importantes e información fundamental relacionada con el marketing en redes sociales:\n",
            "\n",
            "1. **IMI (Instant Marketing Interactions)**:\n",
            "   - **Definición**: Aunque no se proporciona una definición explícita de IMI en el texto, se infiere que se refiere a interacciones de marketing instantáneas, especialmente en el contexto de redes sociales como Twitter.\n",
            "   - **Impacto**: Los IMI pueden influir en métricas perceptuales como la viralidad y en métricas objetivas como los rendimientos de las acciones de una empresa.\n",
            "\n",
            "2. **Viralidad**:\n",
            "   - **Definición**: Número de veces que se comparte un mensaje de marketing, específicamente el volumen de retweets de un tweet.\n",
            "   - **Medición**: Se mide a través del número de retweets.\n",
            "\n",
            "3. **Rendimiento de las Acciones**:\n",
            "   - **Definición**: Los rendimientos anómalos del mercado de valores de una empresa se calculan utilizando el modelo de ci...\n",
            "\n",
            "\n",
            "--- Procesando PDF de Sociología ---\n",
            "Información extraída de Sociología:\n",
            "Basándome exclusivamente en el contexto proporcionado, he identificado y extraído las definiciones clave, conceptos importantes e información fundamental relacionada con la \"sociología\" de las redes sociales:\n",
            "\n",
            "1. **Comunicación y Producción**:\n",
            "   - **Definición**: La comunicación es una forma de trabajo productivo y es el aspecto de la actividad humana que crea significado en las relaciones sociales. La producción humana es social y, por lo tanto, una relación comunicativa.\n",
            "   - **Concepto**: La comunicación no solo es un medio de transmisión de información, sino también una actividad productiva que crea y reproduce significados y relaciones sociales.\n",
            "\n",
            "2. **Control y Diseño de Medios de Comunicación**:\n",
            "   - **Preguntas Clave**: ¿Quién controla los medios de comunicación y para qué se utilizan? ¿Cómo afecta el diseño de los medios de comunicación al desarrollo de habilidades comunicativas y al aprendizaje político?\n",
            "   - **Concepto**: El control y el diseño de los medios de comunicación ...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Proceso de extracción completado. Consulta el archivo 'informacion_extraida_de_pdfs.txt' para ver los resultados.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF para la extracción de texto de PDF\n",
        "from llama_index.core import Document, Settings, VectorStoreIndex\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# --- 0. Configuración de la Clave de API ---\n",
        "# Asegúrate de que tu clave de API de Mistral AI esté configurada como variable de entorno\n",
        "# export MISTRAL_API_KEY=\"TU_CLAVE_MISTRAL_AQUI\"\n",
        "# O, para una prueba rápida (NO recomendado para producción):\n",
        "os.environ[\"MISTRAL_API_KEY\"] = \"MISTRAL_API_KEY\" # ¡Reemplaza con tu clave real!\n",
        "\n",
        "# --- 1. Definiciones Contextuales (opcionales para este flujo, pero útiles si quieres usarlas para guiar la extracción) ---\n",
        "# En este escenario, estas definiciones no son estrictamente necesarias para la extracción del PDF,\n",
        "# pero podrían usarse en prompts más avanzados si el PDF es ambiguo.\n",
        "# Por simplicidad, las mantendremos al margen por ahora, ya que el objetivo es extraer directamente del PDF.\n",
        "# Si los PDFs no son claros, podemos reintroducirlas para \"guiar\" la extracción.\n",
        "\n",
        "# --- 2. Configurar Ajustes de LlamaIndex (LLM y Embeddings) ---\n",
        "# Usamos mistral-large-latest para una mejor comprensión y extracción.\n",
        "Settings.llm = MistralAI(model=\"mistral-large-latest\", temperature=0.1)\n",
        "Settings.embed_model = MistralAIEmbedding(model_name=\"mistral-embed\")\n",
        "\n",
        "# --- 3. Función para Extraer Texto de PDF (de tu código) ---\n",
        "def extraer_texto_pdf(ruta_pdf):\n",
        "    \"\"\"Extrae el texto completo de un documento PDF.\"\"\"\n",
        "    doc = fitz.open(ruta_pdf)\n",
        "    texto_completo = \"\"\n",
        "    for pagina in doc:\n",
        "        texto_completo += pagina.get_text()\n",
        "    return texto_completo\n",
        "\n",
        "# --- 4. Rutas a tus PDFs ---\n",
        "# IMPORTANTE: Asegúrate de que estas rutas sean correctas.\n",
        "ruta_pdf_marketing = \"/content/maketing article.pdf\" # Reemplaza con la ruta real de tu PDF de marketing\n",
        "ruta_pdf_sociologia = \"/content/sevignani-2024-communicative-activity-social-theoretical-foundations-for-critical-materialist-media-and-communication.pdf\" # Reemplaza con la ruta real de tu PDF de sociología\n",
        "\n",
        "\n",
        "# --- 5. Procesar cada PDF y Extraer Información Específica ---\n",
        "\n",
        "extracted_info = {}\n",
        "output_filename = \"informacion_extraida_de_pdfs.txt\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    for doc_type, path in [(\"Marketing\", ruta_pdf_marketing), (\"Sociología\", ruta_pdf_sociologia)]:\n",
        "        print(f\"\\n--- Procesando PDF de {doc_type} ---\")\n",
        "        f_out.write(f\"\\n--- INFORMACIÓN EXTRAÍDA DE {doc_type} ---\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            texto_pdf = extraer_texto_pdf(path)\n",
        "            pdf_document_llama_index = Document(text=texto_pdf, metadata={\"source\": f\"pdf_{doc_type.lower()}\"})\n",
        "\n",
        "            # Crear un VectorStoreIndex para cada PDF para poder hacer consultas sobre su contenido\n",
        "            current_index = VectorStoreIndex.from_documents([pdf_document_llama_index])\n",
        "            query_engine = current_index.as_query_engine(similarity_top_k=3) # Recupera los 3 fragmentos más relevantes\n",
        "\n",
        "            # Prompt para extraer información o definiciones clave\n",
        "            extraction_prompt_tmpl = PromptTemplate(\"\"\"\n",
        "                Eres un asistente experto en análisis de texto.\n",
        "                Basándote EXCLUSIVAMENTE en el siguiente contexto extraído de un documento,\n",
        "                identifica y extrae las definiciones clave, conceptos importantes o información fundamental\n",
        "                relacionada con la \"{topic}\" de las redes sociales.\n",
        "                Si no se encuentra información explícita sobre ello, indícalo.\n",
        "\n",
        "                Contexto del documento:\n",
        "                {context_str}\n",
        "\n",
        "                Información clave extraída (Definiciones, conceptos, ideas principales sobre {topic}):\n",
        "                \"\"\")\n",
        "\n",
        "            # La consulta específica para extraer la información\n",
        "            query_str = f\"Extrae definiciones, conceptos clave e información fundamental sobre {doc_type.lower()} en redes sociales.\"\n",
        "\n",
        "            # Actualiza el prompt del motor de consulta para esta tarea de extracción\n",
        "            query_engine.update_prompts({\"response_synthesizer:text_qa_template\": extraction_prompt_tmpl.partial_format(topic=doc_type.lower())})\n",
        "\n",
        "            # Realizar la consulta de extracción\n",
        "            response = query_engine.query(query_str)\n",
        "            extracted_text = str(response).strip()\n",
        "\n",
        "            extracted_info[doc_type] = extracted_text\n",
        "\n",
        "            print(f\"Información extraída de {doc_type}:\\n{extracted_text[:1000]}...\\n\")\n",
        "            f_out.write(extracted_text + \"\\n\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error al procesar el PDF de {doc_type}: {e}\"\n",
        "            extracted_info[doc_type] = error_message\n",
        "            print(error_message)\n",
        "            f_out.write(error_message + \"\\n\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(f\"Proceso de extracción completado. Consulta el archivo '{output_filename}' para ver los resultados.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OML2caDVm_B6"
      },
      "source": [
        "Luego, en el siguiente codigo separamos cada una de las definiciones y perspectivas de la sociologia y el marketing digital, buscando aplicar separadamente estos conceptos con el documento de analisis de temas.\n",
        "\n",
        "La eleccion metodologica para la separacion de temas se debe a la intencion de mantener separadas las posibles interpretaciones de nuestro modelo de lenguaje con respecto a la aplicacion conceptual. Para ello, de forma general caracterizamos un patron escrito en el documento, y, luego, almacenamos los documentos en dos variables vacias definidas previamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6VsIy9RZm4o3",
        "outputId": "77f1f9f7-1dd6-4f6c-dc28-1f79b0af4327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo 'informacion_extraida_de_pdfs.txt' leído con éxito.\n",
            "\n",
            "--- Contenido de Marketing Extraído (primeros 500 caracteres) ---\n",
            "\n",
            "Basándome exclusivamente en el contexto proporcionado, he identificado y extraído las definiciones clave, conceptos importantes e información fundamental relacionada con el marketing en redes sociales:\n",
            "\n",
            "1. **IMI (Instant Marketing Interactions)**:\n",
            "   - **Definición**: Aunque no se proporciona una definición explícita de IMI en el texto, se infiere que se refiere a interacciones de marketing instantáneas, especialmente en el contexto de redes sociales como Twitter.\n",
            "   - **Impacto**: Los IMI pueden influir en métricas perceptuales como la viralidad y en métricas objetivas como los rendimientos de las acciones de una empresa.\n",
            "\n",
            "2. **Viralidad**:\n",
            "   - **Definición**: Número de veces que se comparte un mensaje de marketing, específicamente el volumen de retweets de un tweet.\n",
            "   - **Medición**: Se mide a través del número de retweets.\n",
            "\n",
            "3. **Rendimiento de las Acciones**:\n",
            "   - **Definición**: Los rendimientos anómalos del mercado de valores de una empresa se calculan utilizando el modelo de cinco factores de Fama-French.\n",
            "\n",
            "4. **Humor**:\n",
            "   - **Definición**: Tweets caracterizados por risa, felicidad o alegría que surgen de juegos de palabras, eventos o imágenes.\n",
            "   - **Impacto**: El humor solo genera viralidad y valor de la empresa cuando se combina con oportunidad o imprevisibilidad.\n",
            "\n",
            "5. **Imprevisibilidad**:\n",
            "   - **Definición**: La forma inesperada en que un tweet responde a un evento externo.\n",
            "\n",
            "6. **Oportunidad**:\n",
            "   - **Definición**: Tiempo que se tarda en responder a un evento externo (medido en minutos).\n",
            "\n",
            "7. **Reputación de la Marca**:\n",
            "   - **Definición**: La presencia de una marca en el ranking Interbrand 100 durante los años 2...\n",
            "\n",
            "--- Contenido de Sociología Extraído (primeros 500 caracteres) ---\n",
            "\n",
            "Basándome exclusivamente en el contexto proporcionado, he identificado y extraído las definiciones clave, conceptos importantes e información fundamental relacionada con la \"sociología\" de las redes sociales:\n",
            "\n",
            "1. **Comunicación y Producción**:\n",
            "   - **Definición**: La comunicación es una forma de trabajo productivo y es el aspecto de la actividad humana que crea significado en las relaciones sociales. La producción humana es social y, por lo tanto, una relación comunicativa.\n",
            "   - **Concepto**: La comunicación no solo es un medio de transmisión de información, sino también una actividad productiva que crea y reproduce significados y relaciones sociales.\n",
            "\n",
            "2. **Control y Diseño de Medios de Comunicación**:\n",
            "   - **Preguntas Clave**: ¿Quién controla los medios de comunicación y para qué se utilizan? ¿Cómo afecta el diseño de los medios de comunicación al desarrollo de habilidades comunicativas y al aprendizaje político?\n",
            "   - **Concepto**: El control y el diseño de los medios de comunicación son cruciales para entender cómo se desarrollan las habilidades comunicativas y el aprendizaje político en diferentes clases sociales.\n",
            "\n",
            "3. **Algoritmos y Tecnología**:\n",
            "   - **Preguntas Clave**: ¿Cómo se diseñan los algoritmos y con qué propósito? ¿Cómo se objetivan en lenguajes de programación, programas, software, y robots comunicativos?\n",
            "   - **Concepto**: Los algoritmos y la tecnología juegan un papel fundamental en la configuración de las relaciones de comunicación social y deben ser analizados críticamente.\n",
            "\n",
            "4. **Teoría de la Actividad**:\n",
            "   - **Definición**: La teoría de la actividad es una escuela de pensamiento que sigue el enfoque cultural-histórico en psicología, representada por figuras como Vygotsky, Luria...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Define la ruta al archivo de entrada\n",
        "nombre_archivo_entrada = \"informacion_extraida_de_pdfs.txt\"\n",
        "\n",
        "# Inicializa las variables para almacenar el contenido extraído\n",
        "documento_marketing_str = \"\"\n",
        "documento_sociologia_str = \"\"\n",
        "\n",
        "try:\n",
        "    # Lee el contenido completo del archivo si existe\n",
        "    if os.path.exists(nombre_archivo_entrada):\n",
        "        with open(nombre_archivo_entrada, \"r\", encoding=\"utf-8\") as f:\n",
        "            contenido_completo_txt = f.read()\n",
        "        print(f\"Archivo '{nombre_archivo_entrada}' leído con éxito.\\n\")\n",
        "    else:\n",
        "        # Si el archivo no existe, lanza una excepción para ser capturada\n",
        "        raise FileNotFoundError(f\"El archivo '{nombre_archivo_entrada}' no se encontró.\")\n",
        "\n",
        "    # --- Expresiones Regulares para Extraer Secciones ---\n",
        "    # Patrón para la sección de Marketing\n",
        "    pattern_marketing = r\"--- INFORMACIÓN EXTRAÍDA DE Marketing ---\\n\\n(.*?)(?=\\n--- INFORMACIÓN EXTRAÍDA DE Sociología ---|\\Z)\"\n",
        "    match_marketing = re.search(pattern_marketing, contenido_completo_txt, re.DOTALL)\n",
        "    # Asignación concisa: si hay coincidencia, toma el grupo 1, si no, una cadena vacía\n",
        "    documento_marketing_str = match_marketing.group(1).strip() if match_marketing else \"\"\n",
        "\n",
        "    # Patrón para la sección de Sociología\n",
        "    pattern_sociologia = r\"--- INFORMACIÓN EXTRAÍDA DE Sociología ---\\n\\n(.*?)(?=\\Z)\"\n",
        "    match_sociologia = re.search(pattern_sociologia, contenido_completo_txt, re.DOTALL)\n",
        "    # Asignación concisa: si hay coincidencia, toma el grupo 1, si no, una cadena vacía\n",
        "    documento_sociologia_str = match_sociologia.group(1).strip() if match_sociologia else \"\"\n",
        "\n",
        "    # Impresión de confirmación (opcional)\n",
        "    if documento_marketing_str:\n",
        "        print(\"--- Contenido de Marketing Extraído (primeros 500 caracteres) ---\\n\")\n",
        "        print(documento_marketing_str[:100000] + \"...\\n\")\n",
        "    else:\n",
        "        print(\"No se pudo extraer la sección 'Marketing'.\")\n",
        "\n",
        "    if documento_sociologia_str:\n",
        "        print(\"--- Contenido de Sociología Extraído (primeros 500 caracteres) ---\\n\")\n",
        "        print(documento_sociologia_str[:100000] + \"...\\n\")\n",
        "    else:\n",
        "        print(\"No se pudo extraer la sección 'Sociología'.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Asegúrate de que el script y el archivo .txt estén en la misma carpeta, o proporciona la ruta completa.\")\n",
        "except Exception as e:\n",
        "    # Captura cualquier otro error que pueda ocurrir durante la lectura o el procesamiento\n",
        "    print(f\"Ocurrió un error inesperado al procesar el archivo: {e}\")\n",
        "\n",
        "# Ahora, documento_marketing_str y documento_sociologia_str contienen la información segmentada.\n",
        "# Puedes usarlas en tu pipeline RAG como `page_content`.\n",
        "# Por ejemplo:\n",
        "# from langchain_core.documents import Document\n",
        "# docs = [\n",
        "#     Document(page_content=documento_marketing_str, metadata={\"source\": \"extracted_marketing\"}),\n",
        "#     Document(page_content=documento_sociologia_str, metadata={\"source\": \"extracted_sociology\"})\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGC7Q7M8ezFV"
      },
      "source": [
        "### Visualizacion del documento de marketing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAbnigjIzYY_",
        "outputId": "7d7f3c5e-848e-4257-87c5-d555a532cc0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basándome exclusivamente en el contexto proporcionado, he identificado y extraído las definiciones clave, conceptos importantes e información fundamental relacionada con el marketing en redes sociales:\n",
            "\n",
            "1. **IMI (Instant Marketing Interactions)**:\n",
            "   - **Definición**: Aunque no se proporciona una definición explícita de IMI en el texto, se infiere que se refiere a interacciones de marketing instantáneas, especialmente en el contexto de redes sociales como Twitter.\n",
            "   - **Impacto**: Los IMI pueden influir en métricas perceptuales como la viralidad y en métricas objetivas como los rendimientos de las acciones de una empresa.\n",
            "\n",
            "2. **Viralidad**:\n",
            "   - **Definición**: Número de veces que se comparte un mensaje de marketing, específicamente el volumen de retweets de un tweet.\n",
            "   - **Medición**: Se mide a través del número de retweets.\n",
            "\n",
            "3. **Rendimiento de las Acciones**:\n",
            "   - **Definición**: Los rendimientos anómalos del mercado de valores de una empresa se calculan utilizando el modelo de cinco factores de Fama-French.\n",
            "\n",
            "4. **Humor**:\n",
            "   - **Definición**: Tweets caracterizados por risa, felicidad o alegría que surgen de juegos de palabras, eventos o imágenes.\n",
            "   - **Impacto**: El humor solo genera viralidad y valor de la empresa cuando se combina con oportunidad o imprevisibilidad.\n",
            "\n",
            "5. **Imprevisibilidad**:\n",
            "   - **Definición**: La forma inesperada en que un tweet responde a un evento externo.\n",
            "\n",
            "6. **Oportunidad**:\n",
            "   - **Definición**: Tiempo que se tarda en responder a un evento externo (medido en minutos).\n",
            "\n",
            "7. **Reputación de la Marca**:\n",
            "   - **Definición**: La presencia de una marca en el ranking Interbrand 100 durante los años 2\n"
          ]
        }
      ],
      "source": [
        "print(documento_marketing_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Ao057qe4S2"
      },
      "source": [
        "###  Visualizacion del documento de sociologia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28MN7yCBzhd4",
        "outputId": "af45509f-62ca-47a0-95c7-0f069b8fd111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basándome exclusivamente en el contexto proporcionado, he identificado y extraído las definiciones clave, conceptos importantes e información fundamental relacionada con la \"sociología\" de las redes sociales:\n",
            "\n",
            "1. **Comunicación y Producción**:\n",
            "   - **Definición**: La comunicación es una forma de trabajo productivo y es el aspecto de la actividad humana que crea significado en las relaciones sociales. La producción humana es social y, por lo tanto, una relación comunicativa.\n",
            "   - **Concepto**: La comunicación no solo es un medio de transmisión de información, sino también una actividad productiva que crea y reproduce significados y relaciones sociales.\n",
            "\n",
            "2. **Control y Diseño de Medios de Comunicación**:\n",
            "   - **Preguntas Clave**: ¿Quién controla los medios de comunicación y para qué se utilizan? ¿Cómo afecta el diseño de los medios de comunicación al desarrollo de habilidades comunicativas y al aprendizaje político?\n",
            "   - **Concepto**: El control y el diseño de los medios de comunicación son cruciales para entender cómo se desarrollan las habilidades comunicativas y el aprendizaje político en diferentes clases sociales.\n",
            "\n",
            "3. **Algoritmos y Tecnología**:\n",
            "   - **Preguntas Clave**: ¿Cómo se diseñan los algoritmos y con qué propósito? ¿Cómo se objetivan en lenguajes de programación, programas, software, y robots comunicativos?\n",
            "   - **Concepto**: Los algoritmos y la tecnología juegan un papel fundamental en la configuración de las relaciones de comunicación social y deben ser analizados críticamente.\n",
            "\n",
            "4. **Teoría de la Actividad**:\n",
            "   - **Definición**: La teoría de la actividad es una escuela de pensamiento que sigue el enfoque cultural-histórico en psicología, representada por figuras como Vygotsky, Luria\n"
          ]
        }
      ],
      "source": [
        "print(documento_sociologia_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3hfHwGnvwl"
      },
      "source": [
        "----\n",
        "Resumen\n",
        "\n",
        "Con la visualizacion de los dos documentos se observa la caracterizacion de las perspectiva de la sociologia centrada en la comunicacion y produccion, el control y diseño de los medios de comunicacion, los algoritmos y la tecnologia, asi como la teoria de la actividad. En el otro caso, desde la perspectiva del marketing digital, se observa conceptos como IMI, viralidad, Rendimiento de acciones, humor, Imprevisibilidad, oportunidad, y reputacion de marca. Con estos dos documentos conceptuales podremos aplicar operativamente nuestros hallazgos con este esquema de referencia teorico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnEjFqzRoXiW"
      },
      "source": [
        "### **Aplicacion operativa de nuestro banco conceptual**\n",
        "\n",
        "En lineas generales el procedimeinto a continuacion se enfoca en:\n",
        "\n",
        "1.  **Preparacion los PDFs:** Colocamos los archivos PDF reales (`documento_marketing_redes_sociales.pdf` y `documento_sociologia_redes_sociales.pdf`) en la ruta especificada en el código (por ejemplo, en el directorio `/content/` porque en este caso usamos Google Colab).\n",
        "2.  **Ejecucion de este script:** Obtendremos un archivo de texto con las definiciones o información que el modelo haya podido extraer de cada PDF sobre marketing y sociología.\n",
        "3.  **Alimentammos el \"código de Gemini\":** Una vez con la informacion extraida en `extracted_info[\"Marketing\"]` y `extracted_info[\"Sociología\"]` (el texto que se guarda en el archivo), usamos esas cadenas de texto como el `texto_largo_a_analizar` resultante del primer código de análisis de palabras clave con Gemini/Mistral.\n",
        "\n",
        "Este enfoque da un control mucho mayor sobre qué partes de los PDFs se utilizan para el análisis posterior. Con esto el modelo Llm podra interpretar el analisis de influencia de los temas aplicando nuestros conceptos de marketing y sociologia.\n",
        "\n",
        "De modo que, posteriormente de instalar las librerias necesarias para acceder a los documentos externos mediante Langchain, y la visualizacion del documento de analisis de influencia creamos nuestro agente de interpretacion conceptual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Dled0fAMnhmJ",
        "outputId": "a6af62f8-68b3-4b0c-9897-945cdd810b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.63)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.5)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma) (1.0.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.6.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (1.72.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain-chroma) (4.24.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb>=1.0.9->langchain-chroma) (0.45.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (0.55b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain-chroma) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.32.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core langchain-community langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BD2udiHgnhn9",
        "outputId": "7ae1a419-db72-4a57-cb46-60d5af1cfb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.63)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NRVWA-w5zmx",
        "outputId": "13a4dc56-fee2-4f0b-9750-37bbff7fd365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pregunta: ¿Cuáles son los 200 temas principales por número de likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Respuesta: Después de analizar la tabla, he identificado los 200 temas principales por número de likes. A continuación, te proporciono la lista con el nombre del tema, el total de likes y el total de shares:\n",
            "\n",
            "**Nota:** La tabla se ordena por el número de likes en orden descendente.\n",
            "\n",
            "1. Social media - 16,026,200 likes - 9,425,000 shares\n",
            "2. Music - 6,070,600 likes - 3,845,200 shares\n",
            "3. Love - 4,810,900 likes - 2,378,300 shares\n",
            "4. Communication - 2,268,580 likes - 1,181,340 shares\n",
            "5. Gratitude - 1,796,760 likes - 1,089,380 shares\n",
            "6. Entertainment - 1,723,750 likes - 1,001,850 shares\n",
            "7. Fashion - 1,321,970 likes - 623,423 shares\n",
            "8. Celebrity - 1,213,020 likes - 692,063 shares\n",
            "9. Relationships - 1,132,930 likes - 704,245 shares\n",
            "10. Relationship - 1,121,030 likes - 610,932 shares\n",
            "... (continúa)\n",
            "\n",
            "**200.** Patriotism - 121,795 likes - 66,615 shares\n",
            "\n",
            "**Observación:** Los temas con un número de likes muy alto son \"Social media\", \"Music\", \"Love\", \"Communication\", \"Gratitude\", \"Entertainment\", \"Fashion\", \"Celebrity\", \"Relationships\" y \"Relationship\".\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuántos posts se asocian a los 200 temas más populares en términos de likes?\n",
            "Respuesta: Para responder a esta pregunta, puedo ordenar la tabla por el número de likes en orden descendente y luego sumar los posts de los 200 temas más populares.\n",
            "\n",
            "Después de ordenar la tabla, puedo ver que los 200 temas más populares en términos de likes son:\n",
            "\n",
            "1. social media - 1,602,620,000 likes\n",
            "2. music - 6,070,600 likes\n",
            "3. love - 4,810,900 likes\n",
            "...\n",
            "200. patriotism - 121,795 likes\n",
            "\n",
            "Luego, puedo sumar los posts de estos 200 temas:\n",
            "\n",
            "1350 + 470 + 230 + ... + 2 = 43,311\n",
            "\n",
            "Por lo tanto, los 200 temas más populares en términos de likes se asocian con un total de 43,311 posts.\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuántos posts se asocian a los 200 temas más populares en términos de shares?\n",
            "Respuesta: Para responder a esta pregunta, puedo ordenar la tabla por el número de shares en orden descendente y luego sumar los posts de los 200 temas más populares.\n",
            "\n",
            "Después de ordenar la tabla, puedo ver que los 200 temas más populares en términos de shares son:\n",
            "\n",
            "1. social media - 9.4255e+06 shares\n",
            "2. music - 3.84532e+06 shares\n",
            "3. love - 2.3783e+06 shares\n",
            "...\n",
            "200. patriotism - 66615 shares\n",
            "\n",
            "Luego, puedo sumar los posts de estos 200 temas:\n",
            "\n",
            "1350 + 470 + 230 + ... + 2 = 14321\n",
            "\n",
            "Por lo tanto, los 200 temas más populares en términos de shares se asocian con un total de 14321 posts.\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuál es el tema con menos shares entre los 200 temas principales por likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Respuesta: Después de analizar la tabla, encontré que el tema con menos shares entre los 200 temas principales por likes es \"anxiety\" con 86,453 shares.\n",
            "\n",
            "Aquí está la información para cada uno de los temas:\n",
            "\n",
            "* Tema: anxiety\n",
            "* Total de likes: 215,548\n",
            "* Total de shares: 86,453\n",
            "\n",
            "Es importante destacar que esta información se basa en la tabla proporcionada y puede no reflejar la realidad total de los temas y sus respectivos engagement.\n",
            "--------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with open(\"/content/llm_responses_engagement_analysis.txt\", 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        print(content)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file was not found at /content/llm_responses_engagement_analysis.txt.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3r2Z-hk6tl9",
        "outputId": "39983bbb-a1d6-4529-d821-a549af6b181c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pregunta: ¿Cuáles son los 200 temas principales por número de likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Respuesta: Después de analizar la tabla, he identificado los 200 temas principales por número de likes. A continuación, te proporciono la lista con el nombre del tema, el total de likes y el total de shares:\n",
            "\n",
            "**Nota:** La tabla se ordena por el número de likes en descenso.\n",
            "\n",
            "1. Social media - 16,026,200 likes - 9,425,000 shares\n",
            "2. Music - 6,070,600 likes - 3,845,200 shares\n",
            "3. Love - 4,810,900 likes - 2,378,300 shares\n",
            "4. Communication - 2,268,580 likes - 1,181,340 shares\n",
            "5. Gratitude - 1,796,760 likes - 1,089,380 shares\n",
            "6. Entertainment - 1,723,750 likes - 1,001,850 shares\n",
            "7. Fashion - 1,321,970 likes - 623,423 shares\n",
            "8. Celebrity - 1,213,020 likes - 692,063 shares\n",
            "9. Relationships - 1,132,930 likes - 704,245 shares\n",
            "10. Relationship - 1,121,030 likes - 610,932 shares\n",
            "... (y así sucesivamente hasta el tema 200)\n",
            "\n",
            "**200.** Patriotism - 121,795 likes - 66,615 shares\n",
            "\n",
            "**Nota:** La tabla completa se puede proporcionar si es necesario.\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuántos posts se asocian al tema más popular en términos de likes?\n",
            "Respuesta: Según la tabla, el tema \"social media\" tiene un total de 1.60262e+07 likes, lo que lo hace el tema más popular en términos de likes. Sin embargo, no se proporciona la cantidad de posts asociados a este tema. La columna \"number_of_posts\" solo proporciona la cantidad de posts para cada tema, pero no se puede determinar cuántos posts se asocian exactamente al tema \"social media\" porque no se proporciona la información necesaria.\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuál es el tema con menos shares entre los 200 temas principales por likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Respuesta: Después de analizar la tabla, encontré que el tema con menos shares entre los 200 temas principales por likes es \"anxiety\" con un total de 86,453 shares.\n",
            "\n",
            "Aquí está la información para \"anxiety\":\n",
            "\n",
            "* Nombre: anxiety\n",
            "* Total de likes: 215,548\n",
            "* Total de shares: 86,453\n",
            "--------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "texto_plano=content\n",
        "print(texto_plano)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbheM6DMVza-"
      },
      "source": [
        "### Análisis e Interpretación con Cadena RAG y Gemini\n",
        "\n",
        "Este código implementa una cadena RAG (Generación Aumentada por Recuperación) utilizando LangChain y modelos de Google Gemini para realizar un análisis profundo e interpretativo de un \"conjunto de palabras\" basándose en conceptos clave de marketing y sociología extraídos de documentos externos.\n",
        "\n",
        "Para ello realizamos los siguientes pasos:\n",
        "\n",
        "- **Preparación del Conocimiento Externo**:\n",
        "Se cargan dos documentos (uno de marketing y otro de sociología) que contienen las definiciones y conceptos clave. Estos documentos se dividen en fragmentos más pequeños (\"chunking\") para una gestión eficiente. Se utilizan embeddings de Google Gemini (models/embedding-001) para convertir estos fragmentos de texto en representaciones numéricas (embeddings), capturando su significado semántico.\n",
        "Estos embeddings se almacenan en una base de datos vectorial (Chroma), que actúa como tu \"biblioteca de conocimiento\" para búsquedas rápidas.\n",
        "Configuración del Modelo de Interpretación:\n",
        "\n",
        "Se inicializa el modelo **`\"gemini-1.5-flash-latest\"`** de Google a través de LangChain, el cual será el encargado de la interpretación final.\n",
        "Diseño del Prompt Experto:\n",
        "\n",
        " - **Creación del Prompt (Plantilla de Instrucciones)**:\n",
        "Se crea una plantilla de prompt detallada. Esta plantilla instruye a Gemini para que actúe como un \"analista experto en marketing y sociología\". Le exige basar su análisis EXCLUSIVAMENTE en el \"contexto\" proporcionado (los conceptos extraídos de tus documentos) y en el \"conjunto de palabras a interpretar\".\n",
        "\n",
        "El prompt guía a Gemini para que estructure su respuesta en un \"informe\" con secciones de \"Análisis de Marketing\" y \"Análisis Sociológico\", relacionando las palabras con los conceptos y explicando la conexión.\n",
        "\n",
        " definimos la estructura y el contenido de las instrucciones que le enviarás al modelo de lenguaje (LLM) de la siguiente forma:\n",
        "\n",
        ">\"Eres un analista experto en marketing y sociología.\n",
        "Basándote EXCLUSIVAMENTE en la información de las secciones de marketing y sociología del contexto,\n",
        "realiza un análisis profundo del conjunto de palabras.\n",
        "Contexto Detallado (Conceptos Clave de Marketing y Sociología):\n",
        "{context} Conjunto de palabras a interpretar:\n",
        "\"{palabras_a_interpretar}\"\n",
        "\n",
        "Luego de la construccion de la plantilla se procedio a la creacion de la cadena RAG. La construccion de la cadena RAG permitiria la interaccion del modelo con nuestros documentos extenos para ofrecernos las respuestas de nnuestras solicitudes segun esos conocimientos especificos.\n",
        "\n",
        "- **Construcción de la Cadena RAG**: La construccion de la cadena RAG seguia las siguientes operaciones. LangChain orquesta la cadena RAG. La cadena se construye para que:\n",
        "\n",
        "- Primero, un retriever (conectado a tu base de datos vectorial) recupera los fragmentos de contexto más relevantes del conocimiento externo (los documentos de marketing y sociología) según el \"conjunto de palabras a interpretar\".\n",
        "- Luego, este contexto recuperado y las \"palabras a interpretar\" se pasan al prompt.El prompt se envía al llm (Gemini).\n",
        "- Finalmente, la respuesta del LLM se procesa con un StrOutputParser para obtener el texto final.\n",
        "\n",
        "La cadena RAG se invoca con el \"conjunto de palabras\" que deseas analizar (texto_plano).  Conjuntamente con esta definicion del modelo empleamos la estructura de prompt ya preparada (prompt_template) y la conectamos con un Modelo de Lenguaje Grande real para crear una secuencia ejecutable, lo que se llama una \"cadena\" otorgandole las siguientes instrucciones:\n",
        "\n",
        ">INSTRUCCIONES DE ANÁLISIS\n",
        "Proporciona un informe estructurado que relacione las palabras con los conceptos claves de cada una de las subsecciones de marketing y sociología de la INFORMACIÓN EXTRAÍDA. Para cada concepto o conclusión, **indica explícitamente qué palabra(s) del \"Conjunto de palabras a interpretar\" te llevaron a esa conexión.**\n",
        "\n",
        "La respuesta generada por Gemini, que contiene el análisis y la interpretación, se imprime en la consola y se guarda en un archivo de texto (analisis_final.txt) para su documentación. Las instrucciones de analisis se aplico tanto para el analisis de marketing como para el analisis sociologico. En ambos analisis se preciso la instruccion de aplicar la informacion del contexto del documento con las definiciones y la identificacion operativa de acuerdo a su frecuencia, shares y likes, y aplica los conceptos relacionados en una conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aLXmepHHnOs9",
        "outputId": "5544c562-5d45-4b5a-8d29-cb51360ae561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ANÁLISIS CON GEMINI ---\n",
            "\n",
            "Analizando: 'Pregunta: ¿Cuáles son los 200 temas principales por número de likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Respuesta: Después de analizar la tabla, he identificado los 200 temas principales por número de likes. A continuación, te proporciono la lista con el nombre del tema, el total de likes y el total de shares:\n",
            "\n",
            "**Nota:** La tabla se ordena por el número de likes en descenso.\n",
            "\n",
            "1. Social media - 16,026,200 likes - 9,425,000 shares\n",
            "2. Music - 6,070,600 likes - 3,845,200 shares\n",
            "3. Love - 4,810,900 likes - 2,378,300 shares\n",
            "4. Communication - 2,268,580 likes - 1,181,340 shares\n",
            "5. Gratitude - 1,796,760 likes - 1,089,380 shares\n",
            "6. Entertainment - 1,723,750 likes - 1,001,850 shares\n",
            "7. Fashion - 1,321,970 likes - 623,423 shares\n",
            "8. Celebrity - 1,213,020 likes - 692,063 shares\n",
            "9. Relationships - 1,132,930 likes - 704,245 shares\n",
            "10. Relationship - 1,121,030 likes - 610,932 shares\n",
            "... (y así sucesivamente hasta el tema 200)\n",
            "\n",
            "**200.** Patriotism - 121,795 likes - 66,615 shares\n",
            "\n",
            "**Nota:** La tabla completa se puede proporcionar si es necesario.\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuántos posts se asocian al tema más popular en términos de likes?\n",
            "Respuesta: Según la tabla, el tema \"social media\" tiene un total de 1.60262e+07 likes, lo que lo hace el tema más popular en términos de likes. Sin embargo, no se proporciona la cantidad de posts asociados a este tema. La columna \"number_of_posts\" solo proporciona la cantidad de posts para cada tema, pero no se puede determinar cuántos posts se asocian exactamente al tema \"social media\" porque no se proporciona la información necesaria.\n",
            "--------------------------------------------------\n",
            "\n",
            "Pregunta: ¿Cuál es el tema con menos shares entre los 200 temas principales por likes? Para cada uno, indica su nombre, el total de likes y el total de shares.\n",
            "Respuesta: Después de analizar la tabla, encontré que el tema con menos shares entre los 200 temas principales por likes es \"anxiety\" con un total de 86,453 shares.\n",
            "\n",
            "Aquí está la información para \"anxiety\":\n",
            "\n",
            "* Nombre: anxiety\n",
            "* Total de likes: 215,548\n",
            "* Total de shares: 86,453\n",
            "--------------------------------------------------\n",
            "\n",
            "'\n",
            "## Análisis del Conjunto de Palabras: Marketing y Sociología\n",
            "\n",
            "El conjunto de palabras presenta datos sobre la popularidad de 200 temas en una plataforma online, medidos por \"likes\" y \"shares\".  Analizaremos estos datos a través de las lentes del marketing y la sociología, basándonos en el contexto proporcionado.\n",
            "\n",
            "**1. Análisis de Marketing:**\n",
            "\n",
            "* **Conceptos Relacionados:**  El contexto de marketing se centra en la comunicación como actividad productiva que crea significado y relaciones sociales.  Los datos proporcionados reflejan directamente métricas de marketing clave.\n",
            "\n",
            "* **Palabras Clave y su Relación con el Marketing:**\n",
            "\n",
            "    * **\"likes\" y \"shares\":** Estas métricas son indicadores fundamentales del *engagement* (compromiso) del usuario.  Un alto número de \"likes\" y \"shares\" indica una campaña exitosa, un contenido atractivo y una alta resonancia con la audiencia.  La lista de los 200 temas, ordenada por número de \"likes\", permite identificar los temas más efectivos en términos de marketing.\n",
            "\n",
            "    * **\"Social media\":**  Como el tema con mayor número de \"likes\" y \"shares\", \"social media\" se posiciona como un tema altamente efectivo en términos de marketing.  Su éxito refleja la importancia de las estrategias de marketing en redes sociales.\n",
            "\n",
            "    * **\"Music\", \"Love\", \"Entertainment\", \"Fashion\":** Estos temas representan nichos de mercado con alto potencial.  Su alta puntuación en \"likes\" y \"shares\" indica una demanda significativa y oportunidades para campañas de marketing dirigidas.\n",
            "\n",
            "    * **\"Communication\":**  La alta puntuación de \"Communication\" refuerza la importancia de la comunicación efectiva en el marketing.  El contexto sociológico refuerza esta idea, al definir la comunicación como una actividad productiva que crea significado.\n",
            "\n",
            "* **Identificación Operativa:** Las palabras más importantes, en términos de marketing, son \"Social media\", \"Music\", \"Love\", \"Entertainment\", \"Fashion\", y \"Communication\", debido a su alto número de \"likes\" y \"shares\".  Estas palabras indican temas con alto potencial de engagement y, por lo tanto, oportunidades de marketing.  La conclusión es que la data revela temas con alta resonancia en la audiencia, ofreciendo valiosas pistas para estrategias de marketing futuras.\n",
            "\n",
            "\n",
            "**2. Análisis Sociológico:**\n",
            "\n",
            "* **Conceptos Relacionados:** El contexto sociológico define la comunicación como una forma de trabajo productivo que crea significado en las relaciones sociales.  La producción humana es social y, por lo tanto, una relación comunicativa.\n",
            "\n",
            "* **Palabras Clave y su Relación con la Sociología:**\n",
            "\n",
            "    * **\"Communication\":**  Este tema es central en el análisis sociológico.  Su alta puntuación en \"likes\" y \"shares\" refleja la importancia de la comunicación en la construcción de relaciones sociales y la creación de significado compartido en la plataforma online.  La alta cantidad de \"likes\" y \"shares\" indica un alto nivel de interacción y construcción de significado colectivo alrededor de este tema.\n",
            "\n",
            "    * **\"Relationships\" y \"Relationship\":**  La presencia de estos términos, con un número significativo de \"likes\" y \"shares\", confirma la importancia de las relaciones interpersonales en la plataforma.  La gente se involucra con contenido que refleja o aborda sus relaciones, confirmando la naturaleza social de la plataforma.\n",
            "\n",
            "    * **\"Love\", \"Gratitude\", \"Patriotism\":**  Estos temas reflejan valores y emociones que son fundamentales para la construcción de identidades sociales y la cohesión social.  Su presencia en la lista indica la importancia de estos aspectos en la vida de los usuarios y su deseo de compartir y conectar con otros a través de estos temas.\n",
            "\n",
            "    * **\"Anxiety\":**  Aunque con menos \"shares\" que otros temas, la presencia de \"Anxiety\" es significativa.  Refleja la vulnerabilidad humana y la necesidad de conectar con otros que comparten experiencias similares, mostrando la función de la plataforma como espacio para la expresión y el apoyo social.\n",
            "\n",
            "* **Identificación Operativa:** Las palabras más importantes desde una perspectiva sociológica son \"Communication\", \"Relationships\", \"Love\", \"Gratitude\", y \"Anxiety\".  Estas palabras revelan la importancia de la comunicación, las relaciones interpersonales, las emociones y la expresión de vulnerabilidades en la construcción de significado y relaciones sociales dentro de la plataforma.  La conclusión es que la plataforma funciona como un espacio de interacción social donde se construyen y comparten significados a través de la comunicación y la expresión de valores y emociones.\n",
            "\n",
            "\n",
            "En resumen, el análisis conjunto de marketing y sociología de los datos revela una imagen rica y compleja de la plataforma online.  El marketing se beneficia de la identificación de temas con alto engagement, mientras que la sociología destaca la importancia de la comunicación, las relaciones y la expresión emocional en la construcción de una comunidad online.\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "Guardando resultados en 'analisis_final.txt'...\n",
            "¡Resultados guardados con éxito!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- 0. Configuración de API Key (¡DE FORMA SEGURA!) ---\n",
        "# Asegúrate de haber configurado tu API key como una variable de entorno\n",
        "# En tu terminal, antes de ejecutar el script, haz:\n",
        "# export GOOGLE_API_KEY=\"AIza...\"\n",
        "# El script la leerá automáticamente. NO la pongas directamente en el código.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\" # <-- No hagas esto en producción\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=documento_marketing_str, metadata={\"source\": \"marketing_definition\"}),\n",
        "    Document(page_content=documento_sociologia_str, metadata={\"source\": \"sociology_definition\"})\n",
        "]\n",
        "\n",
        "# --- 2. Procesamiento de Documentos (Chunking) ---\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# --- 3. Embeddings (Usando GoogleGenerativeAIEmbeddings) ---\n",
        "# CORRECCIÓN 1: Usar el modelo específico para embeddings.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# --- 4. Vector Store (Chroma) ---\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# --- 5. Configuración del Retriever ---\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# --- 6. Creación de la Cadena RAG (Usando ChatGoogleGenerativeAI) ---\n",
        "# CORRECCIÓN 2: Usar un modelo de chat moderno y soportado como gemini-1.5-flash.\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
        "\n",
        "# Diseño del Prompt que guía al LLM.\n",
        "template = \"\"\"\n",
        "Eres un analista experto en marketing y sociología.\n",
        "Basándote EXCLUSIVAMENTE en la información de las secciones de marketing y sociología del contexto, realiza un análisis profundo del conjunto de palabras.\n",
        "\n",
        "Contexto Detallado (Conceptos Clave de Marketing y Sociología):\n",
        "{context}\n",
        "\n",
        "Conjunto de palabras a interpretar:\n",
        "\"{palabras_a_interpretar}\"\n",
        "\n",
        "---\n",
        "**INSTRUCCIONES DE ANÁLISIS:**\n",
        "\n",
        "Proporciona un informe estructurado que relacione las palabras con los conceptos claves de cada una de las subsecciones de marketing y sociología de la INFORMACIÓN EXTRAÍDA.\n",
        "Para cada concepto o conclusión, **indica explícitamente qué palabra(s) del \"Conjunto de palabras a interpretar\" te llevaron a esa conexión.**\n",
        "\n",
        "**1. Análisis de Marketing:**\n",
        "    * **Conceptos Relacionados:** aplica la información de marketing del contexto que son directamente relevantes para las palabras proporcionadas de acuerdo a . Asegúrate de mencionar qué palabras de la lista original (\"{palabras_a_interpretar}\") te hicieron establecer cada relación.\n",
        "    * **Identificacion operativa:** identifica las palabras mas importantes de acuerdo a su frecuencia, shares y likes, y aplica los conceptos relacionados en una conclusion.\n",
        "**2. Análisis Sociológico:**\n",
        "    * **Conceptos Relacionados:** aplica la información sociológica del contexto que son directamente relevantes para las palabras proporcionadas. Asegúrate de mencionar qué palabras de la lista original (\"{palabras_a_interpretar}\") te hicieron establecer cada relación.\n",
        "    * **Identificacion operativa:** identifica las palabras mas importantes de acuerdo a su frecuencia, shares y likes, y aplica los conceptos relacionados en una conclusion.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Construcción de la cadena RAG.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"palabras_a_interpretar\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- 7. Interpretación de Conjuntos de Palabras ---\n",
        "print(\"--- ANÁLISIS CON GEMINI ---\")\n",
        "\n",
        "# Creamos una lista para guardar todos los resultados\n",
        "resultados_completos = []\n",
        "\n",
        "print(f\"\\nAnalizando: '{texto_plano}'\")\n",
        "# Ahora que texto_plano está definido, esta llamada funcionará como se espera.\n",
        "respuesta_1 = rag_chain.invoke(texto_plano)\n",
        "print(respuesta_1)\n",
        "# Añadimos el resultado a nuestra lista\n",
        "resultados_completos.append(f\"Análisis para: '{texto_plano}'\\n{respuesta_1}\\n\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- NUEVO: Guardar todo en un archivo ---\n",
        "print(\"\\nGuardando resultados en 'analisis_final.txt'...\")\n",
        "with open(\"analisis_final.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"--- ANÁLISIS COMPLETOS DE GEMINI ---\\n\\n\")\n",
        "    # Une todos los resultados con una línea de separación\n",
        "    f.write((\"\\n\" + \"=\"*50 + \"\\n\").join(resultados_completos))\n",
        "\n",
        "print(\"¡Resultados guardados con éxito!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3tPCXeisJ91"
      },
      "source": [
        "# **Conclusión: La Intersección de Marketing y Sociología en el Engagement Digital**\n",
        "\n",
        "El análisis de los 200 temas más populares, vistos a través del lente del marketing y la sociología, revela una imagen multifacética de la dinámica de engagement en la plataforma online. Esta doble perspectiva no solo identifica lo que capta la atención del público, sino que también desentraña las razones subyacentes de esa interacción, uniendo el \"qué funciona\" con el \"por qué funciona\".\n",
        "\n",
        "Desde una óptica de marketing, los datos subrayan el poder innegable de ciertos temas para generar engagement masivo. Palabras clave como \"Social media,\" \"Music,\" \"Love,\" \"Entertainment,\" \"Fashion,\" y \"Communication\" emergen como catalizadores de interacciones, evidenciando su alto potencial para campañas dirigidas y la resonancia con amplios segmentos de la audiencia. La preponderancia de \"Social media\" como el tema más popular reafirma la eficacia de las estrategias de marketing digital, mientras que la fuerza de \"Communication\" subraya que el engagement efectivo se cimienta en mensajes claros y significativos. Estos hallazgos proporcionan una hoja de ruta invaluable para optimizar futuras estrategias de contenido y campañas publicitarias.\n",
        "\n",
        "Complementariamente, el análisis sociológico profundiza en la naturaleza de la interacción humana en el espacio digital. Conceptos como \"Communication,\" \"Relationships,\" \"Love,\" \"Gratitude,\" y \"Anxiety\" resaltan la función de la plataforma como un vasto entramado de relaciones sociales. La alta relevancia de la \"Comunicación\" valida la premisa de que la interacción social es una forma de trabajo productivo que genera significado. La resonancia de temas como \"Relationships\" y \"Love\" subraya que la plataforma es un espacio vital para la conexión interpersonal y la expresión de emociones y valores compartidos, mientras que la presencia de \"Anxiety\" indica su rol como foro para la vulnerabilidad y el apoyo mutuo.\n",
        "\n",
        "En síntesis, este estudio conjunto ilustra cómo el éxito de marketing está intrínsecamente ligado a la comprensión sociológica de la interacción humana. Los temas que generan alto engagement no son meros puntos de datos para los mercadólogos; son reflejos de la comunicación, los valores y las relaciones que definen a una comunidad online. La plataforma se consolida así como un ecosistema dinámico donde las estrategias de marketing más efectivas son aquellas que resuenan con las necesidades sociales y emocionales de sus usuarios, construyendo así una comunidad participativa y significativa."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
